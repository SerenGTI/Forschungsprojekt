%!TEX root=../main.tex

\section{Evaluation}
\todo{Einleitung}

\subsection{Environment}
For testing the graph processing systems, we used 5 machines with two AMD EPYC 7401 (24-Cores) and 256 GB of RAM each. One of those machines was only used as part of the distributed cluster, since it only has 128 GB of RAM.
All five machines were running Ubuntu 18.04.2 LTS.

Setup of each framework was performed according to our provided installation guides available in Appendix \ref{app:installationGuides}.
All benchmark cases were initiated by our benchmark script available in our repository.
All five frameworks are tested on a single server.
Galois, Gemini and Giraph were benchmarked in on the distributed 5-node cluster as well.
Since Galois supports this parameter, we ran multiple tests comparing Galois' performance with different thread counts on a single machine.
Furthermore, Galois is a framework capable of utilizing hugepages. We include an evaluation using those on the single node as well.
Unless mentioned otherwise, we always show results of each framework utilizing 96 threads (i.e. the maximum on our machines) for the single-node evaluation.

The complete benchmark log files and extracted raw results are available in our repository\footnote{\url{https://github.com/SerenGTI/Forschungsprojekt} // das ist hier unsch√∂n.}.


\subsection{Data Sets}
The graphs used in our testing can be seen in detail in \autoref{tbl:graphs}. We included a variaty of different graph sizes, from relatively small graphs like the flickr graph with 2 million edges up to an rMat28 with 4.2 billion edges. All graphs except the rMat27 and rMat28 are exemplary real-world graphs and were retrieved from the graph database\footnote{\url{http://konect.uni-koblenz.de/}} associated with the Koblenz Network Collection (KONECT)\cite{konect}.
Both the rMat27 and rMat28 were created with a modified version (we changed the output format to EdgeList) of a graph generator provided by Ligra.
\begin{table}
	\centering
	\caption{Size Comparison of the Used Graphs}
	\begin{tabular}{crr}
		\hline
		\bf{Graph}&\# Vertices (M)&\# Edges (M)\\\hline
		flickr&    		0.1&  2\\
		orkut&          3&    117\\
		wikipedia&      12&   378\\
		twitter&     	52&   1963\\
		rMat27&         63&   2147\\
		friendster&     68&   2586\\
		rMat28&         121&  4294\\
		\hline
	\end{tabular}
	\label{tbl:graphs}
\end{table}


\subsection{Measurements}
For every framework, we measured the \emph{execution time} as the time from start to finish of the console command.
For the \emph{calculation time}, we tried to extract only the time the framework actually executed the algorithm.
Furthermore, the \emph{overhead} is the time difference between execution time and calculation time.
For measuring the calculation time, came up with the following:
\begin{itemize}
	\item For Galois, we extract console log time stamps. Galois outputs \enquote{\texttt{Reading graph complete.}}. Calculation time is the time from this output to the end of execution.

	This is not the most realiable way for measuring the calculation times.
	Not only due to unavoidable buffering in the console output we expect the measured time to be larger than the actual.
	First, it is not clear that all initialization is in fact complete after reading the graph. Second, we include time in the measurement that is used for cleanup after calculation.

	However, this method is the only way of retreiving any measurements without introducing custom modifications to the Galois source code.

	\item Polymer outputs the name of the algorithm followed by an internally measured time.

	\item Gemini outputs a line \texttt{exec\_time=x}, which was used to measure the calculation time.

	\item Ligra outputs its time measurement with \texttt{Running time : x}.

	\item Giraph has built in timers for the iterations (supersteps), the sum of those is the computation time.
\end{itemize}
Each evaluation consisting of graph, framework and algorithm was run 10 times, allowing us to smooth slight variations in the measured times.
Later on, we provide the mean values of the individual times as well as the standard deviation where meaningful.



\subsection{Algorithms}
The three problems Breadth-first search (BFS), PageRank (PR) and Single-source shortest-path (SSSP) were used to benchmark framework with every graph.
We always show the results of PageRank with a maximum of five iterations.
For frameworks that support multiple implementations (i.e. PageRank in push and pull modes), we included both in our evaluation.

In detail, the algorithms for each framework are:
\begin{itemize}
	\item Galois: Galois supports all of our tested algorithms, with both a Push and a Pull variant for PageRank available. In the distributed scenario, there are Push and Pull versions for SSSP and BFS available as well.
	\item Gemini: \todo{}
	\item Giraph: Giraph does not natively supply a BFS algorithm, so in our comparisons a custom implementation is used. For SSSP, slight variations had to be made to the default implementation, to allow us to use different start vertices. For PageRank the supplied implementation is used.
	\item Ligra: \todo{}
	\item Polymer: Polymer suppoerts SSSP based on BellmanFord, BFS and two implementations of PageRank. The two implementations are a regular PR and a Delta Variant.
\end{itemize}
