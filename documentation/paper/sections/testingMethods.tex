%!TEX root=../main.tex

\subsection{Hardware and Software}
For testing all systems we used 5 machines with 96 CPU cores each (48 physical) and 256 GB of RAM. One of those machines was only used as part of the distributed cluster, since it only has 128 GB of RAM.
All servers were running Ubuntu 19.x.
Setup of each framework was performed according to our provided installation guides.

All benchmarks were initiated by our benchmark script that is available in our repository.

The complete log files and extracted raw results will be available in our repository as well.

\subsection{Measurements}
For every framework, we measured the \emph{execution time} as the time from start to finish of the console command.

For the \emph{calculation time}, we tried to extract only the time the framework actually executed the algorithm. We came up with the following:
\begin{itemize}
	\item For Galois, we resulted to extracting console log time stamps. Galois outputs \texttt{Reading graph complete.}. Calculation time is the time from this output to the end of execution.
	\item Polymer outputs the name of the algorithm followed by a measured time.
	\item Gemini outputs a line \texttt{exec\_time=x}, which was used to measure the calculation time. 
	\item Ligra outputs a measured time with \texttt{Running time : x}.
	\item Giraph has built in timers for the iterations (supersteps) which we used to extract the computation time.
\end{itemize}
Each test case consisting of graph, framework and algorithm was run 10 times, allowing us to smooth slight variations in the measured times.

