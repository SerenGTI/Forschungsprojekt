%!TEX root=../main.tex


\subsection{Testing Methods}
The testing methods cover the test environment, including hardware and the setup of the frameworks, the graphs used, followed by the algorithms utilized and our measurement procedures. 

\subsubsection{Environment}
For testing the graph processing systems, we used 5 machines with two AMD EPYC 7401 (24-Cores) and 256 GB of RAM each. Thus, each machine has 96 cores, half of which are virtual. One of those machines was only used as part of the distributed cluster, since it only has 128 GB of RAM.
All five machines were running Ubuntu 18.04.2 LTS.

The setup of each framework was performed according to our provided installation guides available in Appendix \ref{app:installationGuides}.
All benchmark cases were initiated by our benchmark script available in our repository.
All five frameworks are tested on a single server.
Galois, Gemini and Giraph were benchmarked on the distributed 5-node cluster as well.
Since Galois supports this parameter, we ran multiple tests comparing Galois' performance with different thread counts on a single machine.
Furthermore, Galois is a framework capable of utilizing hugepages. We include an evaluation using those on the single node as well.
Unless mentioned otherwise, we always show results of each framework utilizing 96 threads (i.e. the maximum on our machines) for the single-node evaluation.
The complete benchmark log files and extracted raw results are available in our repository\footnote{\url{https://github.com/SerenGTI/Forschungsprojekt}}.


\subsubsection{Data Sets}
The graphs used in our testing can be seen in detail in \autoref{tbl:graphs}. We included a variaty of different graph sizes, from relatively small graphs like the flickr graph with 2 million edges up to an rMat28 with 4.2 billion edges. All graphs except the rMat27 and rMat28 are exemplary real-world graphs and were retrieved from the graph database associated with the Koblenz Network Collection (KONECT)\cite{konect}.
Both the rMat27 and rMat28 were created with a modified version of a graph generator provided by Ligra (we changed the output format to EdgeList).
\begin{table}
	\centering
	\caption{Size Comparison of the Used Graphs}
	\begin{tabular}{crr}
		\hline
		\bf{Graph}&\# Vertices (M)&\# Edges (M)\\\hline
		flickr&    		0.1&  2\\
		orkut&          3&    117\\
		wikipedia&      12&   378\\
		twitter&     	52&   1963\\
		rMat27&         63&   2147\\
		friendster&     68&   2586\\
		rMat28&         121&  4294\\
		\hline
	\end{tabular}
	\label{tbl:graphs}
\end{table}

\subsubsection{Algorithms}
The three problems Breadth-first search (BFS), PageRank (PR) and Single-source shortest-path (SSSP) were used to benchmark each framework with every graph.
We always show the results of PageRank with a maximum of five iterations.
For frameworks that support multiple implementations (i.e. PageRank in push and pull modes), we included both in our evaluation.
We chose SSSP and BFS because they are iterative traversal algorithms. Active vertices typically are locally concentrated in the graph. The results of these algorithms can give some insight on the behaviour of the framework with other, similar behaving algorithms.
PageRank on the other hand is an algorithm that is very different to SSSP or BFS for that matter. With PR, there are many active vertices spread across the entire graph, enforcing different data handling strategies from the framework.

In detail, the algorithms for each framework are:
\begin{itemize}
	\item Ligra supports SSSP based on BellmanFord, BFS and two implementations of PageRank. The two implementations are a regular PR and a Delta Variant.
	\item Polymer supports the same algorithms as Ligra.
	\item Gemini supports all of our tested algorithms. For SSSP a BellmanFord implementation is used.
	\item Galois supports all of our tested algorithms too, with both a Push and a Pull variant for PageRank available. In the distributed scenario, there are Push and Pull versions for SSSP and BFS available as well. It also supports multiple implementations of the shared-memory allgorithms. The default implementation of SSSP is deltaTile. A lot of options are available as well, but we used the defaults.
	\item Giraph does not natively supply a BFS algorithm, so in our comparisons a custom implementation is used. For SSSP, slight variations had to be made to the default implementation, to allow us to use different start vertices. For PageRank the supplied implementation is used.
\end{itemize}


\subsubsection{Measurements}
For every framework, we measured the \emph{execution time} as the time from start to finish of the console command.
For the \emph{calculation time}, we tried to extract only the time the framework actually executed the algorithm.
Furthermore, the \emph{overhead} is the time difference between execution time and calculation time. This includes time to read the input graph, initialization and any other tasks other than the actual user-defined algorithm.
Measuring the execution time is straight forward and was done using console time stamps.
For measuring the calculation time, we came up with the following:
\begin{itemize}
	\item For Galois, we extract console log time stamps. Galois outputs \enquote{\texttt{Reading graph complete.}}. Calculation time is the time from this output to the end of execution.

	This is not the most realiable way of measuring the calculation times.
	Not only due to unavoidable buffering in the console output we expect the measured time to be larger than the actual.
	First, it is not clear that all initialization is in fact complete after reading the graph. Second, we include time in the measurement that is used for cleanup after calculation.

	However, this method is the only way of retreiving any measurements without introducing custom modifications to the Galois source code.

	\item Polymer outputs the name of the algorithm followed by an internally measured time.

	\item Gemini outputs a line \texttt{exec\_time=x}, which was used to measure the calculation time.

	\item Ligra outputs its time measurement with \texttt{Running time : x}.

	\item Giraph has built in timers for the iterations (supersteps), the sum of those is the computation time.
\end{itemize}
Each evaluation consisting of graph, framework and algorithm was run 10 times, allowing us to smooth slight variations in the measured times.
Later on, we provide the mean values of the individual times as well as the standard deviation where meaningful.
