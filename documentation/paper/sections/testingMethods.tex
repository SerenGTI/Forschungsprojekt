%!TEX root=../main.tex

\subsection{Hardware and Software}
For testing all systems we used 5 machines with 96 CPU cores each (48 physical) and 256 GB of RAM. One of those machines was only used as part of the distributed cluster, since it only has 128 GB of RAM.
All servers were running Ubuntu 19.x.
Setup of each framework was performed according to our provided installation guides.

All benchmarks were initiated by our benchmark script that is available in our repository. Galois, Gemini and Giraph were benchmarked on both the 5-node cluster and a singe machine.
Since Galois supports this parameter, we ran multiple tests comparing Galois' performance with different thread counts on a single machine.

The complete benchmark log files and extracted raw results will be available in our repository as well.

\subsection{Measurements}
For every framework, we measured the \emph{execution time} as the time from start to finish of the console command.

For the \emph{calculation time}, we tried to extract only the time the framework actually executed the algorithm. We came up with the following:
\begin{itemize}
	\item For Galois, we resulted to extracting console log time stamps. Galois outputs \texttt{Reading graph complete.}. Calculation time is the time from this output to the end of execution.
	\item Polymer outputs the name of the algorithm followed by a measured time.
	\item Gemini outputs a line \texttt{exec\_time=x}, which was used to measure the calculation time. 
	\item Ligra outputs a measured time with \texttt{Running time : x}.
	\item Giraph has built in timers for the iterations (supersteps) which we used to extract the computation time.
\end{itemize}
Each test case consisting of graph, framework and algorithm was run 10 times, allowing us to smooth slight variations in the measured times.

\subsection{Graphs}
The graphs used in our testing can be seen in detail in \autoref{tbl:graphs}. We included a variaty of different graph sizes, from relatively small graphs like the flickr graph up to an rMat28 with $4.2$ billion edges. All graphs except the rMat27 and rMat28 are exemplary real-world graphs and were retrieved from the graph database\footnote{\url{http://konect.uni-koblenz.de/}} associated with the Koblenz Network Collection (KONECT)\cite{konect}.
Both the rMat27 and rMat28 were created with a modified version (we changed the output format to EdgeList) of a graph generator provided by Ligra.
\begin{table}
	\caption{Size comparison of the used graphs}
	\label{tbl:graphs}
	\centering
	\begin{tabular}{crr}
		\hline
		\bf{Graph}&\# Vertices (M)&\# Edges (M)\\\hline
		flickr&    		0.1&  2\\
		orkut&          3&    117\\
		wikipedia&      12&   378\\
		twitter&     	52&   1963\\
		rMat27&         63&   2147\\
		friendster&     68&   2586\\
		rMat28&         121&  4294\\
		\hline
	\end{tabular}
\end{table}


\subsection{Algorithms}
We ran Breadth-first search (BFS), PageRank (PR) and Single-source shortest-path (SSSP) algorithms on each framework and graph.
For frameworks that support multiple implementations (e.g. PageRank in push and pull modes), we included the alternatives in our testing. 
\autoref{tbl:algorithms} shows the tested algorithms in detail.
\newcommand{\txtdag}{\textsuperscript{\textdagger}}
\begin{table}
	\caption{Tested algorithms of the frameworks}
	\label{tbl:algorithms}
	\centering
	\begin{tabularx}{\columnwidth}{cYYYYY}
		\bf{Algorithm}&\rot{Galois}&\rot{Gemini}&\rot{Giraph}&\rot{Ligra}&\rot{Polymer}\\\hline
		PageRank&$\Delta$Push, $\Delta$Pull&&&&\\
		SSSP&?, Push, Pull&&&&\\
		BFS&Yes&&*&&\\
		\hline
	\end{tabularx}

	* Algorithm not natively supported

\end{table}


\subsection{Comparison of setup}

