%!TEX root=../main.tex

\subsection{Hardware}
For testing the graph processing systems, we used 5 machines with two AMD EPYC 7401 (24-Cores) and 256 GB of RAM each. One of those machines was only used as part of the distributed cluster, since it only has 128 GB of RAM.
All five machines were running Ubuntu 18.04.2 LTS.
Setup of each framework was performed according to our provided installation guides\footnote{The setup guides are available at \url{https://github.com/SerenGTI/Forschungsprojekt/tree/master/documentation}}.

\subsection{Benchmark Software}
All benchmarks were initiated by our benchmark script, that is available in our repository. Galois, Gemini and Giraph were benchmarked on both the 5-node cluster and a singe machine.
Since Galois supports this parameter, we ran multiple tests comparing Galois' performance with different thread counts on a single machine.

The complete benchmark log files and extracted raw results are available in our repository.

\subsection{Data Sets}
The graphs used in our testing can be seen in detail in \autoref{tbl:graphs}. We included a variaty of different graph sizes, from relatively small graphs like the flickr graph with 100 thousand edges up to an rMat28 with $4.2$ billion edges. All graphs except the rMat27 and rMat28 are exemplary real-world graphs and were retrieved from the graph database\footnote{\url{http://konect.uni-koblenz.de/}} associated with the Koblenz Network Collection (KONECT)\cite{konect}.
Both the rMat27 and rMat28 were created with a modified version (we changed the output format to EdgeList) of a graph generator provided by Ligra.
\begin{table}
	\centering
	\caption{Size Comparison of the Used Graphs}
	\begin{tabular}{crr}
		\hline
		\bf{Graph}&\# Vertices (M)&\# Edges (M)\\\hline
		flickr&    		0.1&  2\\
		orkut&          3&    117\\
		wikipedia&      12&   378\\
		twitter&     	52&   1963\\
		rMat27&         63&   2147\\
		friendster&     68&   2586\\
		rMat28&         121&  4294\\
		\hline
	\end{tabular}
	\label{tbl:graphs}
\end{table}


\subsection{Measurements}
For every framework, we measured the \emph{execution time} as the time from start to finish of the console command.

For the \emph{calculation time}, we tried to extract only the time the framework actually executed the algorithm. We came up with the following:
\begin{itemize}
	\item For Galois, we resulted to extracting console log time stamps. Galois outputs \texttt{Reading graph complete.}. Calculation time is the time from this output to the end of execution.

	We know that this is not the most realiable way for measuring the calculation times. 
	Not only due to unavoidable buffering in the console output we expect the measured time to be larger than the actual.
	First, it is not clear that all initialization is in fact complete after reading the graph. Second, we include time that is used for cleanup after calculation in the measurement.

	\item Polymer outputs the name of the algorithm followed by an internally measured time.

	\item Gemini outputs a line \texttt{exec\_time=x}, which was used to measure the calculation time.

	\item Ligra outputs its time measurement with \texttt{Running time : x}.

	\item Giraph has built in timers for the iterations (supersteps) which we summed up to extract the computation time.
\end{itemize}
Furthermore, the \emph{overhead} is the time difference between execution time and calculation time.

Each test case consisting of graph, framework and algorithm was run 10 times, allowing us to smooth slight variations in the measured times.
Later on, we provide the mean values of the individual times as well as the standard deviation where meaningful.



\subsection{Algorithms}
The three problems Breadth-first search (BFS), PageRank (PR) and Single-source shortest-path (SSSP) were used to benchmark framework with every graph.
For frameworks that support multiple implementations (e.g. PageRank in push and pull modes), we included the alternatives in our testing. 
\autoref{tbl:algorithms} shows the tested algorithms in detail.

\newcommand{\txtdag}{\textsuperscript{\textdagger}}
\begin{table}
	\renewcommand{\arraystretch}{1.3}
	\centering
	\caption{Tested Algorithms of Each Framework}
	\begin{tabularx}{0.9\columnwidth}{cYYYYY}
		\bf{Algorithm}&Galois&Gemini&Giraph&Ligra&Polymer\\\hline
		PageRank&$\Delta$Push, $\Delta$Pull&&&&Regular, $\Delta$\\\hline
		SSSP&?, Push, Pull&&&&\\\hline
		BFS&Yes&&Yes*&&\\
		\hline
	\end{tabularx}
	\label{tbl:algorithms}

	{\vspace*{1ex}* Algorithm not natively supported

	\txtdag{} Modified algorithm

	\textsuperscript{d} Single node and distributed
	}

\end{table}


\subsection{Comparison of setup}

