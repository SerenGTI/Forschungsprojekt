%!TEX root=../../main.tex

\subsection{Results}
Ahead of the discussion of the results a few of the issues we encountered while testing are mentioned. Afterwards the benchmark results for SSSP, BFS, PR and the speedup of Galois given a varying number of threads are discussed in that order.


\subsubsection{Encountered Issues}
We would like to raise some issues we encountered first while installing and configuring and second while running the different frameworks.

\begin{itemize}
	\item During setup and benchmark of Gemini, we encountered several bugs in the cloned repository. These include non zero-terminated strings or even missing return statements.

	The errors rendered the code as-is unable to perform calculations, forcing us to fork the repository and to modify the source code. Our changes can be found in one of our repositories\footnote{\url{https://github.com/jasc7636/GeminiGraph}}.

	\item Furthermore, we would like to address the setup of Hadoop for Giraph. It requires multiple edits in \texttt{xml} files that aren't easily automized. This makes the setup rather time consuming, especially if reconfiguration is needed later on.
	\item In order for Giraph to run, several Java tasks (the Hadoop infrastructure) have to be constantly running in the background. While we don't expect this to have a significant performance impact on other tasks, it is still suboptimal.
	\item Giraph ran us into disk space problems on multiple occasions. First, deleting files on the Hadoop distributed file system (HDFS) does not immediately free up disk space because the files are moved to a \emph{recycling bin}-like location. Second, some log files that can easily be multiple gigabytes in size are stored outside of the HDFS and are never mentioned in the Giraph documentation.
\end{itemize}
\noindent
Setup of the frameworks Polymer and Ligra was straight forward and did not require any special treatment.
Galois is also fairly easy to setup without hugepages and the process is well documented. 
Setting up hugepages might require some research and a bit of trial and error to find the ideal way, since there are several ways to achive the same.





\input{sections/results/sssp}

\input{sections/results/bfs}

\input{sections/results/pr}

\subsubsection{Comparison of a Single-Node to the Distributed Cluster}
\begin{table}
	\caption{Execution Times in Seconds on a Single Node vs. 5-Node Distributed Cluster}
	\label{tbl:execTimeComparison}
	\renewcommand{\arraystretch}{1.2}
	\centering
	\begin{tabular}{ccr@{\tabskip 1 \tabcolsep}r
	r@{\tabskip 1 \tabcolsep}r
	r@{\tabskip 1 \tabcolsep}r}
		\toprule
		&&\multicolumn{2}{c}{\bf Galois}&\multicolumn{2}{c}{\bf Gemini}&\multicolumn{2}{c}{\bf Giraph}\\
		\cmidrule{3-4}\cmidrule{5-6}\cmidrule{7-8}
		&\bf Graph&1N&5N&1N&5N&1N&5N\\
		\midrule
		\multirow{7}{0.5ex}{\rotatebox{90}{\bf SSSP}}&flickr & \bf 0.3 & 2.4 & \bf 0.6 & 2.5 & \bf 38.8 & 42.6 \\
		& orkut & \bf 0.8 & 4.2 & \bf 13.9 & 17.3 & 135.9 & \bf 60.4 \\
		& wikipedia & \bf 1.8 & 14.2 & \bf 48.5 & 74.5 & 377.2 & \bf 100.8 \\
		& twitter & \bf 10.8 & 40.0 & \bf 383.0 & 391.1 & - & \bf 349.9 \\
		& rMat27 & \bf 16.0 & 39.0 & 563.5 & \bf 386.0 & - & \bf 565.8 \\
		& friendster & \bf 14.4 & 58.5 & 742.6 & \bf 568.9 & - & \bf 444.0 \\
		& rMat28 & \bf 27.8 & 71.5 & 1236.7 & \bf 792.0 & - & \bf 1180.2 \\
		\midrule
		\multirow{7}{0.5ex}{\rotatebox{90}{\bf BFS}}& flickr & \bf 0.6 & 2.3 & \bf 0.7 & 2.4 & \bf 38.9 & 41.9 \\
		& orkut & \bf 0.9 & 4.0 & \bf 12.5 & 13.8 & 128.2 & \bf 58.1 \\
		& wikipedia & \bf 2.4 & 13.2 & \bf 44.9 & 60.9 & 360.7 & \bf 95.1 \\
		& twitter & \bf 14.2 & 37.9 & 355.0 & \bf 293.2 & - & \bf 322.3 \\
		& rMat27 & \bf 14.6 & 34.4 & 540.8 & \bf 305.3 & - & \bf 539.2 \\
		& friendster & \bf 12.8 & 47.6 & 708.4 & \bf 450.2 & - & \bf 412.3 \\
		& rMat28 & \bf 33.1 & 67.1 & 1178.7 & \bf 634.3 & - & \bf 1135.6 \\
		\midrule
		\multirow{7}{0.5ex}{\rotatebox{90}{\bf PR}}& flickr & \bf 0.3\txtdagger & 2.6 & \bf 0.7 & 2.2 & 45.6 & \bf 44.1 \\
		& orkut & \bf 0.7\txtdagger & 8.0 & \bf 13.1 & 19.4 & 561.3 & \bf 106.2 \\
		& wikipedia & \bf 1.7\txtdagger & 206.5 & \bf 44.4 & 54.4 & 1501.3 & \bf 252.6 \\
		& twitter & \bf 8.7\txtdagger & 910.9 & \bf 359.9 & 363.5 & - & \bf 1200.2 \\
		& rMat27 & \bf 19.2\txtdagger & 1287.4 & 536.3 & \bf 376.3 & - & \bf 1369.5 \\
		& friendster & \bf 20.4\txtdagger & 593.3 & 716.5 & \bf 591.1 & - & \bf 1655.0 \\
		& rMat28 & \bf 46.0\txtdagger & 2540.2 & 1188.5 & \bf 774.2 & - & - \\
		\bottomrule
		\multicolumn{8}{l}{(\txtdagger) Results of Galois Pull shown.}
	\end{tabular}
\end{table}
Until now, we have only compared the frameworks with each other, under the same setup circumstances. Hence, a comparison of the same framework, in single-node versus distributed setup is the content of this section.
Of course only frameworks that were tested in both setups are shown.
We focus primarily on the results of the execution times, the data can be seen in \autoref{tbl:execTimeComparison}. We show the results of Galois Push for the 5-node cluster in the table, since it is faster than Pull.

There are some, rather large differences in the execution times of each framework. Galois is consistently faster on a single computation node compared to the distributed setup. 
On both SSSP and BFS, the margin between the two setups is already noticeable. The distributed scenario requires from 2$\times$ (BFS, rMat28) to 7.8$\times$ (SSSP, wikipedia) the execution time of single-node Galois.
For PR however, the difference is multiple orders of magnitude large. Distributed Galois requires from 8.6$\times$ to 121$\times$ more time than single-node Galois. 

Gemini's distributed calculation only requires more time on the smaller graphs (i.e. flickr to wikipedia). There, the distributed scenario is anywhere from 1.1$\times$ (BFS, orkut) to 4.2$\times$ (SSSP, flickr) slower.
Twitter is the tipping point for Gemini's SSSP and PR algorithm. Here, execution times on the single-node and distributed cluster are within 2\% of each other. For BFS, the tipping point is anywhere between twitter and rMat27.
Above that, i.e. on the larger graphs, the added computation power can be leveraged to overcome the synchronization overhead of the distributed scenario. Hence, on those graphs single-node Gemini becomes up to 1.8$\times$ (BFS, rMat28) slower than the distributed version.
This relation is very dependent on the graph size. The smaller the graph, the faster single-node Gemini is in comparison. Analogously, the larger the graph, the larger the gap becomes, in favour of the distributed version.

A very similiar behaviour can be observed for Giraph, but the relation tips in favour of the distributed version much sooner. Giraph already uses the distributed cluster efficiently on the smaller graphs as well.
On flickr, the execution times of single-node vs distributed Giraph are already within 9\% of each other.
On the larger graphs, the differences quickly increase, again in favour of the distributed scenario. Giraph's single-node version runs up to 5.9$\times$ (PR, wikipedia) slower than the distributed version.
Keep in mind, that only a comparison between the three smallest graphs can be made here.

\input{sections/results/galoisSpeedup}

